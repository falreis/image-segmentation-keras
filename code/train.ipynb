{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, argparse, numpy as np, math, sys, copy\n",
    "from skimage.segmentation import slic, mark_boundaries, felzenszwalb\n",
    "from skimage.util import img_as_float\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import superpixels as sp\n",
    "import glob\n",
    "import pipeline as pipe\n",
    "import time\n",
    "import superpixels as sp\n",
    "import scipy.stats as sci\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import LoadBatches\n",
    "from Models import Segnet, Unet, VGGSegnet, VGGUnet, FCN8, FCN32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train path and batch size\n",
    "train_images_path = '/media/falreis/falreis1/mestrado/deep_learning/BSDS500/pre_process/X/images/'\n",
    "train_segs_path = '/media/falreis/falreis1/mestrado/deep_learning/BSDS500/pre_process/X/colorTruth/'\n",
    "train_batch_size = 2 #args.batch_size\n",
    "\n",
    "#val path and batch size\n",
    "val_images_path = '/media/falreis/falreis1/mestrado/deep_learning/BSDS500/pre_process/images/val/'\n",
    "val_segs_path = '/media/falreis/falreis1/mestrado/deep_learning/BSDS500/pre_process/colorTruth/val/'\n",
    "val_batch_size = 2 #args.val_batch_size\n",
    "\n",
    "#weights\n",
    "save_weights_path = '/media/falreis/falreis1/mestrado/deep_learning/weights/'\n",
    "load_weights = '' #args.load_weights\n",
    "\n",
    "#input height and width\n",
    "input_height = 481 #args.input_height\n",
    "input_width = 481 #args.input_width\n",
    "\n",
    "#classes, validade and epochs\n",
    "n_classes = 2 #args.n_classes\n",
    "validate = 'store_false' #args.validate\n",
    "epochs = 10 # args.epochs\n",
    "\n",
    "#optmizer and model name\n",
    "optimizer_name = 'adadelta' #args.optimizer_name\n",
    "model_name = 'segnet' #args.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape (None, 3840, 2)\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 108s 2s/step - loss: 0.6150 - acc: 0.6744 - val_loss: 6.4979 - val_acc: 0.0095\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 105s 2s/step - loss: 0.0759 - acc: 0.9887 - val_loss: 6.4125 - val_acc: 0.0886\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 105s 2s/step - loss: 0.0198 - acc: 0.9951 - val_loss: 7.1700 - val_acc: 0.1017\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 105s 2s/step - loss: 0.0229 - acc: 0.9949 - val_loss: 7.2290 - val_acc: 0.1279\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 105s 2s/step - loss: 0.0014 - acc: 0.9972 - val_loss: 7.2324 - val_acc: 0.1636\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 105s 2s/step - loss: 3.7775e-04 - acc: 0.9971 - val_loss: 6.1412 - val_acc: 0.2713\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 107s 2s/step - loss: 1.7603e-04 - acc: 0.9971 - val_loss: 7.2339 - val_acc: 0.1426\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 107s 2s/step - loss: 1.0808e-04 - acc: 0.9971 - val_loss: 6.6269 - val_acc: 0.1495\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 104s 2s/step - loss: 7.6607e-05 - acc: 0.9971 - val_loss: 7.3519 - val_acc: 0.1716\n",
      "Epoch 1/1\n",
      "64/64 [==============================] - 105s 2s/step - loss: 5.8838e-05 - acc: 0.9971 - val_loss: 8.3408 - val_acc: 0.0735\n"
     ]
    }
   ],
   "source": [
    "modelFns = { 'segnet':Segnet.Segnet\n",
    "            , 'unet':Unet.Unet\n",
    "            , 'vgg_segnet':VGGSegnet.VGGSegnet\n",
    "            , 'vgg_unet':VGGUnet.VGGUnet\n",
    "            , 'vgg_unet2':VGGUnet.VGGUnet2\n",
    "            , 'fcn8':FCN8.FCN8\n",
    "            , 'fcn32':FCN32.FCN32   \n",
    "}\n",
    "modelFN = modelFns[ model_name ]\n",
    "\n",
    "m = modelFN(n_classes , input_height=input_height, input_width=input_width)\n",
    "m.compile(loss='categorical_crossentropy', optimizer= optimizer_name, metrics=['accuracy'])\n",
    "\n",
    "if len( load_weights ) > 0:\n",
    "    m.load_weights(load_weights)\n",
    "\n",
    "print(\"Model output shape\" ,  m.output_shape)\n",
    "\n",
    "output_height = m.outputHeight\n",
    "output_width = m.outputWidth\n",
    "\n",
    "G  = LoadBatches.imageSegmentationGenerator(train_images_path\n",
    "                                            , train_segs_path\n",
    "                                            , train_batch_size\n",
    "                                            , n_classes\n",
    "                                            , input_height\n",
    "                                            , input_width\n",
    "                                            , output_height\n",
    "                                            , output_width\n",
    ")\n",
    "\n",
    "if validate:\n",
    "    G2  = LoadBatches.imageSegmentationGenerator(val_images_path\n",
    "                                                 , val_segs_path\n",
    "                                                 , val_batch_size\n",
    "                                                 , n_classes\n",
    "                                                 , input_height\n",
    "                                                 , input_width\n",
    "                                                 , output_height\n",
    "                                                 , output_width\n",
    ")\n",
    "\n",
    "if not validate:\n",
    "    for ep in range(epochs):\n",
    "        m.fit_generator(G, 64, epochs=1)\n",
    "        m.save_weights(save_weights_path + \".\" + str(ep))\n",
    "        m.save(save_weights_path + \".model.\" + str(ep))\n",
    "else:\n",
    "    for ep in range(epochs):\n",
    "        m.fit_generator(G, 64, validation_data=G2, validation_steps=100, epochs=1)\n",
    "        m.save_weights(save_weights_path + \".\" + str(ep))\n",
    "        m.save(save_weights_path + \".model.\" + str(ep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
