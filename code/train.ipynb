{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.5 |Anaconda, Inc.| (default, Mar 29 2018, 18:21:58) \n",
      "[GCC 7.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt, argparse, numpy as np, math, sys, copy\n",
    "from skimage.segmentation import slic, mark_boundaries, felzenszwalb\n",
    "from skimage.util import img_as_float\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "import superpixels as sp\n",
    "import glob\n",
    "import pipeline as pipe\n",
    "import time\n",
    "import superpixels as sp\n",
    "import scipy.stats as sci\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import LoadBatches\n",
    "from Models import Segnet, Unet, VGGSegnet, VGGUnet, FCN8, FCN32\n",
    "from keras.optimizers import SGD\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train path and batch size\n",
    "train_images_path = '/media/falreis/falreis2/mestrado/deep_learning/BSDS500/pre_process/X/images/train/'\n",
    "train_segs_path = '/media/falreis/falreis2/mestrado/deep_learning/BSDS500/pre_process/X/colorTruth/train/'\n",
    "train_batch_size = 2 #args.batch_size\n",
    "\n",
    "#val path and batch size\n",
    "val_images_path = '/media/falreis/falreis2/mestrado/deep_learning/BSDS500/pre_process/X/images/val/'\n",
    "val_segs_path = '/media/falreis/falreis2/mestrado/deep_learning/BSDS500/pre_process/X/colorTruth/val/'\n",
    "val_batch_size = 2 #args.val_batch_size\n",
    "\n",
    "#weights\n",
    "save_weights_path = '/media/falreis/falreis2/mestrado/deep_learning/weights/'\n",
    "load_weights = '' #'.model.4' #args.load_weights\n",
    "\n",
    "#input height and width\n",
    "input_height = 360 #481 #args.input_height\n",
    "input_width = 480 #481 #args.input_width\n",
    "\n",
    "#classes, validade and epochs\n",
    "n_classes = 2 #args.n_classes\n",
    "validate = 'store_false' #args.validate\n",
    "epochs = 2 #args.epochs\n",
    "\n",
    "#optmizer and model name\n",
    "#optimizer_name = 'adam' #args.optimizer_name\n",
    "optimizer = SGD(lr=0.001, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model_name = 'segnet' #args.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape (None, 172800, 2)\n"
     ]
    }
   ],
   "source": [
    "modelFns = { 'segnet':Segnet.Segnet\n",
    "            , 'unet':Unet.Unet\n",
    "            , 'vgg_segnet':VGGSegnet.VGGSegnet\n",
    "            , 'vgg_unet':VGGUnet.VGGUnet\n",
    "            , 'vgg_unet2':VGGUnet.VGGUnet2\n",
    "            , 'fcn8':FCN8.FCN8\n",
    "            , 'fcn32':FCN32.FCN32   \n",
    "}\n",
    "modelFN = modelFns[ model_name ]\n",
    "\n",
    "m = modelFN(n_classes , input_height=input_height, input_width=input_width)\n",
    "m.compile(loss='categorical_crossentropy', optimizer= optimizer, metrics=['accuracy'])\n",
    "\n",
    "if len( load_weights ) > 0:\n",
    "    m.load_weights(save_weights_path + load_weights)\n",
    "\n",
    "print(\"Model output shape\" ,  m.output_shape)\n",
    "\n",
    "output_height = m.outputHeight\n",
    "output_width = m.outputWidth\n",
    "\n",
    "validate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-67d894dd8c81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "G  = LoadBatches.imageSegmentationGenerator(train_images_path\n",
    "                                            , train_segs_path\n",
    "                                            , train_batch_size\n",
    "                                            , n_classes\n",
    "                                            , input_height\n",
    "                                            , input_width\n",
    "                                            , output_height\n",
    "                                            , output_width\n",
    ")\n",
    "\n",
    "if validate:\n",
    "    G2  = LoadBatches.imageSegmentationGenerator(val_images_path\n",
    "                                                 , val_segs_path\n",
    "                                                 , val_batch_size\n",
    "                                                 , n_classes\n",
    "                                                 , input_height\n",
    "                                                 , input_width\n",
    "                                                 , output_height\n",
    "                                                 , output_width\n",
    ")\n",
    "\n",
    "if not validate:\n",
    "    for ep in range(epochs):\n",
    "        m.fit_generator(G, 32, nb_epoch=1)\n",
    "        m.save_weights(save_weights_path + \".\" + str(ep))\n",
    "        m.save(save_weights_path + \".model.\" + str(ep))\n",
    "else:\n",
    "    for ep in range(epochs):\n",
    "        m.fit_generator(G, 32, validation_data=G2, validation_steps=10, nb_epoch=1)\n",
    "        m.save_weights(save_weights_path + \".\" + str(ep))\n",
    "        m.save(save_weights_path + \".model.\" + str(ep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input and output images\n",
    "images_path='/media/falreis/falreis2/mestrado/deep_learning/BSDS500/pre_process/X/images/val/'\n",
    "output_path='/media/falreis/falreis2/mestrado/deep_learning/BSDS500/predict/'\n",
    "\n",
    "epoch_number = 2 #epochs - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88, 8, 2)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images = glob.glob(images_path + \"*.jpg\") + glob.glob(images_path + \"*.png\") \n",
    "images.sort()\n",
    "\n",
    "for imgName in images:\n",
    "    outName = imgName.replace(images_path, output_path)\n",
    "    X = LoadBatches.getImageArr(imgName, input_width, input_height)\n",
    "    output = m.predict_proba(np.array([X]))\n",
    "    output = output.reshape((output.shape[0], output_height, output_width, n_classes))\n",
    "    \n",
    "    print(output[0].shape)\n",
    "    \n",
    "    fig, (ax1) = plt.subplots(1,1, figsize=(20, 20))\n",
    "    ax1.imshow(np.argmax(output[0], axis=-1))\n",
    "    ax1.set_axis_off()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = glob.glob( images_path + \"*.jpg\"  ) + glob.glob( images_path + \"*.png\"  ) +  glob.glob( images_path + \"*.jpeg\"  )\n",
    "images.sort()\n",
    "\n",
    "colors = [  ( random.randint(0,255),random.randint(0,255),random.randint(0,255)   ) for _ in range(n_classes)  ]\n",
    "\n",
    "for imgName in images:\n",
    "    outName = imgName.replace(images_path, output_path )\n",
    "    X = LoadBatches.getImageArr(imgName, input_width, input_height)\n",
    "    pr = m.predict(np.array([X]))[0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    pr = pr.reshape(( output_height ,  output_width , n_classes ) ).argmax( axis=2 )\n",
    "    seg_img = np.zeros( ( output_height , output_width , 3  ) )\n",
    "    \n",
    "    for c in range(n_classes):\n",
    "        seg_img[:,:,0] += ( (pr[:,: ] == c )*( colors[c][0] )).astype('uint8')\n",
    "        seg_img[:,:,1] += ((pr[:,: ] == c )*( colors[c][1] )).astype('uint8')\n",
    "        seg_img[:,:,2] += ((pr[:,: ] == c )*( colors[c][2] )).astype('uint8')\n",
    "    seg_img = cv2.resize(seg_img  , (input_width , input_height ))\n",
    "      \n",
    "    cv2.imwrite(  outName , seg_img )\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import utils\n",
    "utils.plot_model(m, to_file='model.png', show_shapes=False, show_layer_names=True, rankdir='TB')\n",
    "#keras.utils.to_categorical(y, num_classes=None)\n",
    "#plot(m, to_file='model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
